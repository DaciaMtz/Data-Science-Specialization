# Summary statistics
all_data %>%
group_by(source) %>%
summarise(
num_lines = n(),
num_words = sum(str_count(text, "\\S+"))
)
# Basic preprocessing
all_data$text <- all_data$text %>%
str_to_lower() %>%
str_replace_all("[^[:alpha:][:space:]]", "") %>%
str_squish()
# Word counts and tokenization
tidy_data <- all_data %>%
unnest_tokens(word, text)
# Top words (excluding stopwords)
data("stop_words")
tidy_data %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
head(20) %>%
ggplot(aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Top Words", x = "Words", y = "Frequency")
# Sampling the data:
# Parameters for sampling
set.seed(123)  # Set seed for reproducibility
sample_probability <- 0.1  # Adjust the probability as needed (e.g., 10%)
# Generate a logical vector for sampling (1 = include, 0 = exclude)
sample_selection <- rbinom(length(all_data), 1, sample_probability)
# Subset the data based on the sampling vector
sample_data <- all_data[sample_selection == 1]
# Sampling the data:
# Parameters for sampling
set.seed(123)  # Set seed for reproducibility
sample_probability <- 0.1  # Adjust the probability as needed (e.g., 10%)
# Generate a logical vector for sampling (1 = include, 0 = exclude)
sample_selection <- rbinom(length(all_data), 1, sample_probability)
# Convert all_data to data.table if it's not already
setDT(all_data)
# Subset the data based on the sample_selection
subset_data <- all_data[sample_selection == 1]
# Sampling the data:
# Parameters for sampling
set.seed(123)  # Set seed for reproducibility
sample_probability <- 0.1  # Adjust the probability as needed (e.g., 10%)
# Generate a logical vector for sampling (1 = include, 0 = exclude)
sample_selection <- rbinom(length(all_data), 1, sample_probability)
# Subset the data based on the sampling vector
sample_data <- all_data[sample_selection == 1,]
# Word counts and tokenization
all_data <- all_data %>%
unnest_tokens(word, text)
library(data.table)
blogs <- fread("~/Coursera_Specialization/Coursera-SwiftKey/en_US/en_US.blogs.txt", sep = "\n", header = FALSE, encoding = "UTF-8", fill = TRUE)
news <- fread("~/Coursera_Specialization/Coursera-SwiftKey/en_US/en_US.news.txt", sep = "\n", header = FALSE, encoding = "UTF-8", fill = TRUE)
twitter <- fread("~/Coursera_Specialization/Coursera-SwiftKey/en_US/en_US.twitter.txt", sep = "\n", header = FALSE, encoding = "UTF-8", fill = TRUE)
# Check the first few rows
head(news,4)
library(data.table)
blogs <- fread("~/Coursera_Specialization/Coursera-SwiftKey/en_US/en_US.blogs.txt", sep = "\n", header = FALSE, encoding = "UTF-8", fill = TRUE)
news <- fread("~/Coursera_Specialization/Coursera-SwiftKey/en_US/en_US.news.txt", sep = "\n", header = FALSE, encoding = "UTF-8", fill = TRUE)
twitter <- fread("~/Coursera_Specialization/Coursera-SwiftKey/en_US/en_US.twitter.txt", sep = "\n", header = FALSE, encoding = "UTF-8", fill = TRUE)
# Check the first few rows
head(news,4)
# Libraries
library(dplyr)
library(ggplot2)
library(tidytext)
library(stringr)
# Combine data
all_data <- bind_rows(
blogs %>% rename(text = V1) %>% mutate(source = "blogs"),
news %>% rename(text = V1) %>% mutate(source = "news"),
twitter %>% rename(text = V1) %>% mutate(source = "twitter")
)
# Summary statistics
all_data %>%
group_by(source) %>%
summarise(
num_lines = n(),
num_words = sum(str_count(text, "\\S+"))
)
# Basic preprocessing
tidy_data$text <- all_data$text %>%
str_to_lower() %>%
str_replace_all("[^[:alpha:][:space:]]", "") %>%
str_squish()
# Sampling the data:
# Parameters for sampling
set.seed(123)  # Set seed for reproducibility
sample_probability <- 0.1  # Adjust the probability as needed (e.g., 10%)
# Generate a logical vector for sampling (1 = include, 0 = exclude)
sample_selection <- rbinom(length(tidy_data), 1, sample_probability)
# Libraries
library(dplyr)
library(ggplot2)
library(tidytext)
library(stringr)
# Combine data
all_data <- bind_rows(
blogs %>% rename(text = V1) %>% mutate(source = "blogs"),
news %>% rename(text = V1) %>% mutate(source = "news"),
twitter %>% rename(text = V1) %>% mutate(source = "twitter")
)
# Summary statistics
all_data %>%
group_by(source) %>%
summarise(
num_lines = n(),
num_words = sum(str_count(text, "\\S+"))
)
# Basic preprocessing
all_data$text <- all_data$text %>%
str_to_lower() %>%
str_replace_all("[^[:alpha:][:space:]]", "") %>%
str_squish()
# Word counts and tokenization
tidy_data <- all_data %>%
unnest_tokens(word, text)
# Top words (excluding stopwords)
data("stop_words")
tidy_data %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
head(20) %>%
ggplot(aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Top Words", x = "Words", y = "Frequency")
# Subsample the data (10% of the data)
set.seed(123)  # For reproducibility
sample_selection <- rbinom(nrow(all_data), size = 1, prob = 0.1)  # 10% chance
all_data_subsample <- all_data[sample_selection == 1, ]
# Subsample the data (10% of the data)
set.seed(123)  # For reproducibility
sample_selection <- rbinom(nrow(all_data), size = 1, prob = 0.1)  # 10% of the data
subsample_data <- all_data[sample_selection == 1, ]
# Unigrams
unigrams <- subsample_data %>%
count(word) %>%
arrange(desc(n))  # Sort by frequency
View(tidy_data)
View(tidy_data)
View(subsample_data)
# Unigrams
unigrams <- tidy_data %>%
count(word) %>%
arrange(desc(n))  # Sort by frequency
total_unigrams <- sum(unigrams$n) # Total number of unigrams
# Calculate probabilities for unigrams
unigram_probs <- unigrams %>%
mutate(prob = n / total_unigrams)
# Bigrams
bigrams <- subsample_data %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
# Count bigrams and calculate probabilities
bigram_counts <- bigrams %>%
count(bigram) %>%
arrange(desc(n))
total_bigrams <- sum(bigram_counts$n) # Total number of bigrams
bigram_probs <- bigram_counts %>%
mutate(prob = n / total_bigrams)
# Trigrams
trigrams <- subsample_data %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3)
# Count trigrams and calculate probabilities
trigram_counts <- trigrams %>%
count(trigram) %>%
arrange(desc(n))
total_trigrams <- sum(trigram_counts$n) # Total number of trigrams
# Calculate probabilities for trigrams
trigram_probs <- trigram_counts %>%
mutate(prob = n / total_trigrams)
# View probabilities
head(unigram_probs)
head(bigram_probs)
head(trigram_probs)
library(dplyr)
library(tidyr)
# Store the trigram model as a Markov Chain
trigram_markov_chain <- trigram_probs %>%
group_by(w1, w2) %>%
summarise(prob_matrix = list(prob), .groups = 'drop')
library(dplyr)
library(tidyr)
library(data.table)
# Create a transition table with probabilities for each trigram
trigram_probs <- trigram_counts %>%
mutate(prob = n / sum(n)) %>%
separate(trigram, into = c("w1", "w2", "w3"), sep = " ") %>%
select(w1, w2, w3, prob)
# Inspect the trigram probabilities
head(trigram_probs)
# Store the trigram model as a Markov Chain
trigram_markov_chain <- trigram_probs %>%
group_by(w1, w2) %>%
summarise(prob_matrix = list(prob), .groups = 'drop')
# The 'prob_matrix' column will store transition probabilities as lists
# Saving the model (save to file for later use)
saveRDS(trigram_markov_chain, "trigram_markov_chain.rds")
# Store the bigram model
bigram_markov_chain <- bigram_probs %>%
group_by(w1) %>%
summarise(prob_matrix = list(prob), .groups = 'drop')
# Basic preprocessing
all_data$text <- all_data$text %>%
filter(!is.na()) %>%   # Remove rows where 'text' is NA
str_to_lower() %>%
str_replace_all("[^[:alpha:][:space:]]", "") %>%
str_squish()
# Basic preprocessing with NA removal
all_data$text <- all_data %>%
filter(!is.na(text)) %>%   # Remove rows where 'text' is NA
pull(text) %>%             # Extract the 'text' column as a vector
str_to_lower() %>%         # Convert to lowercase
str_replace_all("[^[:alpha:][:space:]]", "") %>%  # Remove non-alphabetic characters
str_squish()               # Remove extra whitespace
# Remove NA values
all_data <- all_data %>% filter(!is.na(text))
# Word counts and tokenization
tidy_data <- all_data %>%
unnest_tokens(word, text)
# Top words (excluding stopwords)
data("stop_words")
tidy_data %>%
anti_join(stop_words, by = "word") %>%
count(word, sort = TRUE) %>%
head(20) %>%
ggplot(aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Top Words", x = "Words", y = "Frequency")
```
# Subsample the data (10% of the data)
set.seed(123)  # For reproducibility
sample_selection <- rbinom(nrow(all_data), size = 1, prob = 0.1)  # 10% of the data
subsample_data <- all_data[sample_selection == 1, ]
# Trigrams
trigrams <- subsample_data %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3)
# Count trigrams and calculate probabilities
trigram_counts <- trigrams %>%
count(trigram) %>%
arrange(desc(n))
total_trigrams <- sum(trigram_counts$n) # Total number of trigrams
# Calculate probabilities for trigrams
trigram_probs <- trigram_counts %>%
mutate(prob = n / total_trigrams)
head(trigram_probs)
# Remove NA values
all_data <- all_data %>% filter(!is.na(text & text != ""))
# Trigrams
trigrams <- subsample_data %>%
unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
filter(!is.na(trigram))
# Count trigrams and calculate probabilities
trigram_counts <- trigrams %>%
count(trigram) %>%
arrange(desc(n))
total_trigrams <- sum(trigram_counts$n) # Total number of trigrams
# Calculate probabilities for trigrams
trigram_probs <- trigram_counts %>%
mutate(prob = n / total_trigrams)
head(trigram_probs)
# Libraries
library(dplyr)
library(ggplot2)
library(tidytext)
library(stringr)
# Combine data
all_data <- bind_rows(
blogs %>% rename(text = V1) %>% mutate(source = "blogs"),
news %>% rename(text = V1) %>% mutate(source = "news"),
twitter %>% rename(text = V1) %>% mutate(source = "twitter")
)
# Summary statistics
all_data %>%
group_by(source) %>%
summarise(
num_lines = n(),
num_words = sum(str_count(text, "\\S+"))
)
# Basic preprocessing with NA removal
all_data$text <- all_data %>%
filter(!is.na(text)) %>%   # Remove rows where 'text' is NA
pull(text) %>%             # Extract the 'text' column as a vector
str_to_lower() %>%         # Convert to lowercase
str_replace_all("[^[:alpha:][:space:]]", "") %>%  # Remove non-alphabetic characters
str_squish()               # Remove extra whitespace
# Remove rows with NA or empty text before creating n-grams
clean_data <- all_data %>% filter(!is.na(text) )
# Word counts and tokenization
tidy_data <- all_data %>%
unnest_tokens(word, text)
library(data.table)
blogs <- fread("en_US.blogs.txt", sep = "\n", header = FALSE, encoding = "UTF-8", fill = TRUE)
news <- fread("en_US.news.txt", sep = "\n", header = FALSE, encoding = "UTF-8", fill = TRUE)
twitter <- fread("en_US.twitter.txt", sep = "\n", header = FALSE, encoding = "UTF-8", fill = TRUE)
# Libraries
library(dplyr)
library(ggplot2)
library(tidytext)
library(stringr)
# Combine data
all_data <- bind_rows(
blogs %>% rename(text = V1) %>% mutate(source = "blogs"),
news %>% rename(text = V1) %>% mutate(source = "news"),
twitter %>% rename(text = V1) %>% mutate(source = "twitter")
)
# Summary statistics
all_data %>%
group_by(source) %>%
summarise(
num_sentences = n(),
num_words = sum(str_count(text, "\\S+"))
)
# Load the saved models
unigram_probs <- readRDS("unigram_probs.rds")
bigram_probs <- readRDS("bigram_probs.rds")
trigram_probs <- readRDS("trigram_probs.rds")
fourgram_probs <- readRDS("fourgram_probs.rds")
# View probabilities
print("Unigrams")
head(unigram_probs)
print("Bigrams")
head(bigram_probs)
print("Trigrams")
head(trigram_probs)
print("Fourgrams")
head(fourgram_probs)
# Filter the data tables for minimum frequency
min_frequency <- 5
fourgram_probs <- fourgram_probs[n >= min_frequency]
View(fourgram_probs)
# Filter the data tables for minimum frequency
min_frequency <- 5
fourgram_probs <- fourgram_probs[n >= min_frequency]
# Filter the data tables for minimum frequency
min_frequency <- 5
fourgram_probs <- fourgram_probs[n >= min_frequency]
trigram_probs <- trigram_probs[n >= min_frequency]
View(bigram_probs)
library(tidyr)
library(data.table)
library(dplyr)
# Filter the data tables for minimum frequency
min_frequency <- 5
fourgram_probs <- fourgram_probs[n >= min_frequency]
trigram_probs <- trigram_probs[n >= min_frequency]
library(tidyr)
library(data.table)
library(dplyr)
# Filter the data tables for minimum frequency
min_frequency <- 5
fourgram_probs <- fourgram_probs[n >= min_frequency]
trigram_probs <- trigram_probs[n >= min_frequency]
# Load the saved models
unigram_probs <- readRDS("unigram_probs.rds")
bigram_probs <- readRDS("bigram_probs.rds")
trigram_probs <- readRDS("trigram_probs.rds")
fourgram_probs <- readRDS("fourgram_probs.rds")
# View probabilities
print("Unigrams")
head(unigram_probs)
print("Bigrams")
head(bigram_probs)
print("Trigrams")
head(trigram_probs)
print("Fourgrams")
head(fourgram_probs)
# Filter the data tables for minimum frequency
min_frequency <- 4
bigram_probs <- bigram_probs[n >= min_frequency]
trigram_probs <- trigram_probs[n >= min_frequency]
fourgram_probs <- fourgram_probs[n >= min_frequency]
# Save the probabilities
saveRDS(bigram_probs, "bigram_probs_filtered.rds")
saveRDS(trigram_probs, "trigram_probs_filtered.rds")
saveRDS(fourgram_probs, "fourgram_probs_filtered.rds")
# Test prediction
input_text = "Id give anything to see arctic monkeys this"
predict_next_word(input_text, fourgram_probs, trigram_probs, bigram_probs, unigram_probs, lambda = c(0.4, 0.3, 0.2, 0.1))
predict_next_word <- function(input, fourgram_probs, trigram_probs, bigram_probs, unigram_probs, lambda = c(0.4, 0.3, 0.2, 0.1)) {
# Tokenization
tokens <- str_split(input, "\\s+")[[1]]
n_tokens <- length(tokens)
# Initialize the result list
result_list <- list()
# Prediction using backoff strategy
if (n_tokens >= 3) {
fourgram_prob <- fourgram_probs %>%
filter(w1 == tokens[n_tokens - 2], w2 == tokens[n_tokens - 1], w3 == tokens[n_tokens]) %>%
transmute(w3 = w4, prob, level = "fourgram")
if (nrow(fourgram_prob) > 0) result_list[[length(result_list) + 1]] <- fourgram_prob
}
if (n_tokens >= 2) {
trigram_prob <- trigram_probs %>%
filter(w1 == tokens[n_tokens - 1], w2 == tokens[n_tokens]) %>%
transmute(w3, prob, level = "trigram")
if (nrow(trigram_prob) > 0) result_list[[length(result_list) + 1]] <- trigram_prob
}
if (n_tokens >= 1) {
bigram_prob <- bigram_probs %>%
filter(w1 == tokens[n_tokens]) %>%
transmute(w3 = w2, prob, level = "bigram")
if (nrow(bigram_prob) > 0) result_list[[length(result_list) + 1]] <- bigram_prob
}
# Unigram prediction (for any input)
unigram_prob <- unigram_probs %>%
transmute(w3 = word, prob = prob * lambda[4], level = "unigram")
result_list[[length(result_list) + 1]] <- unigram_prob
# Combine all predictions
combined <- bind_rows(result_list) %>%
group_by(w3) %>%
summarise(prob = sum(prob, na.rm = TRUE), .groups = "drop") %>%
arrange(desc(prob))
# Return the 3 words with the highest combined probability
return(head(combined$w3, 3))
}
# Test prediction
input_text = "Id give anything to see arctic monkeys this"
predict_next_word(input_text, fourgram_probs, trigram_probs, bigram_probs, unigram_probs, lambda = c(0.4, 0.3, 0.2, 0.1))
# Test prediction
library(stringr)
input_text = "Id give anything to see arctic monkeys this"
predict_next_word(input_text, fourgram_probs, trigram_probs, bigram_probs, unigram_probs, lambda = c(0.4, 0.3, 0.2, 0.1))
shiny::runApp('Coursera_Specialization/Shiny Application/Capstone_Project_App')
shiny::runApp()
runApp()
runApp()
library(dplyr)
library(tidyr)
library(data.table)
library(stringr)
# Load the saved models
unigram_probs <- readRDS("unigram_probs.rds")
bigram_probs <- readRDS("bigram_probs_filtered.rds")
trigram_probs <- readRDS("trigram_probs_filtered.rds")
fourgram_probs <- readRDS("fourgram_probs_filtered.rds")
library(dplyr)
library(tidyr)
library(data.table)
library(stringr)
# Load the saved models
unigram_probs <- readRDS("unigram_probs.rds")
bigram_probs <- readRDS("bigram_probs_filtered.rds")
trigram_probs <- readRDS("trigram_probs_filtered.rds")
fourgram_probs <- readRDS("fourgram_probs_filtered.rds")
generate_predictions <- function(input, fourgram_probs, trigram_probs, bigram_probs, unigram_probs) {
lambda = c(0.4, 0.3, 0.2, 0.1)
# Tokenization
tokens <- str_split(input, "\\s+")[[1]]
n_tokens <- length(tokens)
# Initialize the result list
result_list <- list()
# Prediction using backoff strategy
if (n_tokens >= 3) {
fourgram_prob <- fourgram_probs %>%
filter(w1 == tokens[n_tokens - 2], w2 == tokens[n_tokens - 1], w3 == tokens[n_tokens]) %>%
transmute(w3 = w4, prob, level = "fourgram")
if (nrow(fourgram_prob) > 0) result_list[[length(result_list) + 1]] <- fourgram_prob
}
if (n_tokens >= 2) {
trigram_prob <- trigram_probs %>%
filter(w1 == tokens[n_tokens - 1], w2 == tokens[n_tokens]) %>%
transmute(w3, prob, level = "trigram")
if (nrow(trigram_prob) > 0) result_list[[length(result_list) + 1]] <- trigram_prob
}
if (n_tokens >= 1) {
bigram_prob <- bigram_probs %>%
filter(w1 == tokens[n_tokens]) %>%
transmute(w3 = w2, prob, level = "bigram")
if (nrow(bigram_prob) > 0) result_list[[length(result_list) + 1]] <- bigram_prob
}
# Unigram prediction (for any input)
unigram_prob <- unigram_probs %>%
transmute(w3 = word, prob = prob * lambda[4], level = "unigram")
result_list[[length(result_list) + 1]] <- unigram_prob
# Combine all predictions
combined <- bind_rows(result_list) %>%
group_by(w3) %>%
summarise(prob = sum(prob, na.rm = TRUE), .groups = "drop") %>%
arrange(desc(prob))
# Return the 3 words with the highest combined probability
return(head(combined$w3, 3))
}
predictions <- generate_predictions(user_input, fourgram_probs, trigram_probs, bigram_probs, unigram_probs)
user_input = "Id give anything to see arctic monkeys this"
predictions <- generate_predictions(user_input, fourgram_probs, trigram_probs, bigram_probs, unigram_probs)
predictions[1]
View(bigram_probs)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
View(generate_predictions)
# Reactive value to store predictions
predictions <- reactiveVal(c())
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
install.packages("shinythemes")
runApp()
library(rsconnect)
deployApp()
