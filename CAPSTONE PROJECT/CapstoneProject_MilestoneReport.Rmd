---
title: "Milestone Report"
author: "Dacia Martinez Diaz"
date: "2024-12-06"
output:
  pdf_document: default
  html_document: default
---

## Introduction

The goal of this task is to explore the dataset, clean the data, and build a preliminary predictive model. The data is sourced from three text files: blogs, news articles, and Twitter posts. In this report, I will provide an overview of the steps taken to process the data, highlight key findings, and outline the next steps toward building a prediction algorithm and a Shiny app.

## Step 0: Importing Data

The first step was importing the datasets. I used the `fread()` function from the `data.table` package to read in the three files:

- **Blogs**
- **News**
- **Twitter**

```{r warning=FALSE}
library(data.table)

blogs <- fread("~/Coursera_Specialization/Coursera-SwiftKey/en_US/en_US.blogs.txt", sep = "\n", header = FALSE, encoding = "UTF-8", fill = TRUE)

news <- fread("~/Coursera_Specialization/Coursera-SwiftKey/en_US/en_US.news.txt", sep = "\n", header = FALSE, encoding = "UTF-8", fill = TRUE)

twitter <- fread("~/Coursera_Specialization/Coursera-SwiftKey/en_US/en_US.twitter.txt", sep = "\n", header = FALSE, encoding = "UTF-8", fill = TRUE)
```

Each dataset was read as a table, with a single column of text data (one line per observation). The encoding was set to "UTF-8" to ensure correct character representation. Here's an example of the first few rows of the twitter dataset:

```{r, echo=FALSE}
# Check the first few rows of the twitter dataset
library(knitr)
kable(head(twitter, 5), format = "markdown")
```
## Step 1: Exploratory Data Analysis (EDA)
### 1. Basic Statistics
The first step in the exploratory data analysis (EDA) was to compute basic statistics, such as word counts and sentence counts, for each dataset. This helped us understand the scale of the data.
```{r, echo=FALSE, warning=FALSE, message=FALSE}
# Libraries
library(dplyr)
library(ggplot2)
library(tidytext)
library(stringr)

# Combine data
all_data <- bind_rows(
  blogs %>% rename(text = V1) %>% mutate(source = "blogs"),
  news %>% rename(text = V1) %>% mutate(source = "news"),
  twitter %>% rename(text = V1) %>% mutate(source = "twitter")
)

# Summary statistics
all_data %>%
  group_by(source) %>%
  summarise(
    num_sentences = n(),
    num_words = sum(str_count(text, "\\S+"))
  )
```
### 2. Data Cleaning
A basic preprocessing was performed to prepare the text for analysis. This included the following steps:

- Lowercase all text for consistency.
- Remove punctuation, numbers, and special characters to focus on words.
- Remove empty lines and extra white space.

```{r}
# Basic preprocessing 
all_data$text <- all_data %>%
  filter(!is.na(text)) %>%   # Remove rows where 'text' is NA
  pull(text) %>%             # Extract the 'text' column as a vector
  str_to_lower() %>%         # Convert to lowercase
  str_replace_all("[^[:alpha:][:space:]]", "") %>%  # Remove non-alphabetic characters
  str_squish()               # Remove extra whitespace

# Remove rows with NA
all_data <- all_data %>% filter(!is.na(text) )
```

### 3. Word Frequency Analysis
The text was tokenized into individual words to do some work frecuency analysis. The next figure displays the most frequent words excluding common stopwords such as "the" and "and". This gives us insight into the most frequently used terms in the blogs, news, and Twitter posts and the key themes in the dataset.

```{r, echo=FALSE}
# Word counts and tokenization
tidy_data <- all_data %>%
  unnest_tokens(word, text) %>% 
  filter(!is.na(word))  # Remove any NA tokens

# Top words (excluding stopwords)
data("stop_words")

tidy_data %>%
  anti_join(stop_words, by = "word") %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Top Words", x = "Words", y = "Frequency")
```

## Step 2: Build N-Gram Models

N-grams of varying lengths (unigrams, bigrams, trigrams, and fourgrams) were constructed and probabilities were calculated for each N-gram to enable prediction.

```{r, eval=FALSE, include =FALSE}

# Subsample the data
set.seed(123)  # For reproducibility
sample_selection <- rbinom(nrow(all_data), size = 1, prob = 0.1)  # 10% of the data
subsample_data <- all_data[sample_selection == 1, ]

library(tidyr)

# Unigrams -------------------------------------
unigrams <- tidy_data %>%
  count(word) %>%
  arrange(desc(n))  # Sort by frequency

unigram_probs <- unigrams %>%
  mutate(prob = n / sum(n))

# Bigrams  -------------------------------------
bigrams <- subsample_data %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))

# Count bigrams and calculate probabilities
bigram_counts <- bigrams %>%
  count(bigram) %>%
  arrange(desc(n))

# Bigram probabilities
bigram_probs <- bigram_counts %>%
    separate(bigram, into = c("w1", "w2"), sep = " ") %>%
    group_by(w1) %>%
    mutate(prob = n / sum(n)) %>%
    ungroup()

# Trigrams  -------------------------------------
trigrams <- subsample_data %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  filter(!is.na(trigram))

# Count trigrams and calculate probabilities
trigram_counts <- trigrams %>%
  count(trigram) %>%
  arrange(desc(n))

# Trigram probabilities
trigram_probs <- trigram_counts %>%
    separate(trigram, into = c("w1", "w2", "w3"), sep = " ") %>%
    group_by(w1, w2) %>%
    mutate(prob = n / sum(n)) %>%
    ungroup()

# Fourgrams -------------------------------------
fourgrams <- subsample_data %>%
  unnest_tokens(fourgram, text, token = "ngrams", n = 4) %>%
  filter(!is.na(fourgram))

# Count fourgrams and calculate probabilities
fourgram_counts <- fourgrams %>%
  count(fourgram) %>%
  arrange(desc(n))

# Fourgram probabilities
fourgram_probs <- fourgram_counts %>%
    separate(fourgram, into = c("w1", "w2", "w3", "w4"), sep = " ") %>%
    group_by(w1, w2, w3) %>%
    mutate(prob = n / sum(n)) %>%
    ungroup()

# Load data.table for faster data manipulation
library(data.table)
# Convert data frames to data.tables
fourgram_probs <- as.data.table(fourgram_probs)
trigram_probs <- as.data.table(trigram_probs)
bigram_probs <- as.data.table(bigram_probs)
unigram_probs <- as.data.table(unigram_probs)

# Save the probabilities
saveRDS(unigram_probs, "unigram_probs.rds")
saveRDS(bigram_probs, "bigram_probs.rds")
saveRDS(trigram_probs, "trigram_probs.rds")
saveRDS(fourgram_probs, "fourgram_probs.rds")

```
```{r, echo=FALSE}
# Load the saved models
unigram_probs <- readRDS("unigram_probs.rds")
bigram_probs <- readRDS("bigram_probs.rds")
trigram_probs <- readRDS("trigram_probs.rds")
fourgram_probs <- readRDS("fourgram_probs.rds")

# View probabilities 
print("Unigrams")
head(unigram_probs)
print("Bigrams")
head(bigram_probs)
print("Trigrams")
head(trigram_probs)
print("Fourgrams")
head(fourgram_probs)
```

# Step 3: Build prediction model

Using the calculated N-gram probabilities, a prediction function was created to suggest the next word based on input text. The function implements a backoff strategy, leveraging N-grams of descending lengths when a match isn't found at higher levels.

```{r, echo = FALSE}

# Filter the data tables for minimum frequency
min_frequency <- 10
fourgram_probs <- fourgram_probs[n >= min_frequency]
trigram_probs <- trigram_probs[n >= min_frequency]

```

```{r, echo = FALSE}
predict_next_word <- function(input, fourgram_probs, trigram_probs, bigram_probs, unigram_probs, lambda = c(0.4, 0.3, 0.2, 0.1)) {
    # Tokenization
    tokens <- str_split(input, "\\s+")[[1]]
    n_tokens <- length(tokens)
    
    # Initialize the result list
    result_list <- list()
    
    # Prediction using backoff strategy
    if (n_tokens >= 3) {
        fourgram_prob <- fourgram_probs %>%
            filter(w1 == tokens[n_tokens - 2], w2 == tokens[n_tokens - 1], w3 == tokens[n_tokens]) %>%
            transmute(w3 = w4, prob, level = "fourgram")
        if (nrow(fourgram_prob) > 0) result_list[[length(result_list) + 1]] <- fourgram_prob
    }
    
    if (n_tokens >= 2) {
        trigram_prob <- trigram_probs %>%
            filter(w1 == tokens[n_tokens - 1], w2 == tokens[n_tokens]) %>%
            transmute(w3, prob, level = "trigram")
        if (nrow(trigram_prob) > 0) result_list[[length(result_list) + 1]] <- trigram_prob
    }
    
    if (n_tokens >= 1) {
        bigram_prob <- bigram_probs %>%
            filter(w1 == tokens[n_tokens]) %>%
            transmute(w3 = w2, prob, level = "bigram")
        if (nrow(bigram_prob) > 0) result_list[[length(result_list) + 1]] <- bigram_prob
    }
    
    # Unigram prediction (for any input)
    unigram_prob <- unigram_probs %>%
        transmute(w3 = word, prob = prob * lambda[4], level = "unigram")
    result_list[[length(result_list) + 1]] <- unigram_prob
    
    # Combine all predictions
    combined <- bind_rows(result_list) %>%
        group_by(w3) %>%
        summarise(prob = sum(prob, na.rm = TRUE), .groups = "drop") %>%
        arrange(desc(prob))
    
    # Return the 3 words with the highest combined probability
    return(head(combined$w3, 3))
}

```

Here is an example of the implementation of the prediction function

```{r}
# Test prediction
input_text = "You're the reason why I smile everyday. Can you follow me please? It would mean the"

predict_next_word(input_text, fourgram_probs, trigram_probs, bigram_probs, unigram_probs, lambda = c(0.4, 0.3, 0.2, 0.1))
```

## Results and Next Steps
#### Results
- Successfully loaded and processed datasets (blogs, news, Twitter).
- Constructed and analyzed N-grams (unigrams, bigrams, trigrams, and fourgrams).
- Developed a prediction function using N-gram probabilities.

#### Next Steps
- Evaluate and optimize the model's performance for real-time text prediction.
- Integrate the prediction model into a Shiny app.

## Conclusion
This exploratory analysis demonstrates that the data has been successfully loaded and cleaned. The next step is to implement a real-time prediction algorithm that can make use of the processed text data. The Shiny app will allow users to interact with the model and see real-time results.
